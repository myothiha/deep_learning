{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connect Neural Network (FCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.1283, 0.6496]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(in_features=input_tensor.shape[1], out_features=num_out)\n",
    "linear(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2842,  0.2831]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequential = nn.Sequential(\n",
    "    nn.Linear(in_features=input_tensor.shape[1], out_features=5),\n",
    "    nn.Linear(in_features=5, out_features=3),\n",
    "    nn.Linear(in_features=3, out_features=num_out)\n",
    ")\n",
    "\n",
    "sequential(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "softmax = nn.Softmax()\n",
    "relu = nn.ReLU()\n",
    "leakyRelu = nn.LeakyReLU(negative_slope=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid (Binary Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8808, 0.9999, 0.2689]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = torch.tensor([[2.0, 9.9, -1]])\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(input_tensor)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5724]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequential = nn.Sequential(\n",
    "    nn.Linear(in_features=input_tensor.shape[1], out_features=5),\n",
    "    nn.Linear(in_features=5, out_features=3),\n",
    "    nn.Linear(in_features=3, out_features=1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "sequential(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax (Multiclass Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.7060e-04, 9.9961e-01, 1.8451e-05],\n",
       "        [3.7060e-04, 9.9961e-01, 1.8451e-05]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = torch.tensor([[2.0, 9.9, -1], [2.0, 9.9, -1], ])\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "softmax(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1182, 0.8818],\n",
       "        [0.1182, 0.8818]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequential = nn.Sequential(\n",
    "    nn.Linear(in_features=input_tensor.shape[1], out_features=5),\n",
    "    nn.Linear(in_features=5, out_features=3),\n",
    "    nn.Linear(in_features=3, out_features=num_out),\n",
    "    nn.Softmax()\n",
    ")\n",
    "\n",
    "sequential(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLu, Elu\n",
    "- ReLU most popular activation function but suffer dying neurons problems where a neuron dies when it outputs negative.\n",
    "    - Prone to vanishing gradient.\n",
    "- Elu solve this problem.\n",
    "    - Less prone to vanishing gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU Usage\n",
    "\n",
    "# sequential = nn.Sequential(\n",
    "#     nn.Linear(in_features=input_tensor.shape[1], out_features=5),\n",
    "#     nn.Linear(in_features=5, out_features=3),\n",
    "#     nn.Linear(in_features=3, out_features=num_out),\n",
    "#     nn.ReLU()\n",
    "# )\n",
    "# sequential(input_tensor)\n",
    "\n",
    "# (or)\n",
    "# sequential = nn.Sequential(\n",
    "#     nn.Linear(in_features=input_tensor.shape[1], out_features=5),\n",
    "#     nn.Linear(in_features=5, out_features=3),\n",
    "#     nn.Linear(in_features=3, out_features=num_out),\n",
    "# )\n",
    "# output = sequential(input_tensor)\n",
    "# yhat = nn.functional.relu(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU Usage\n",
    "\n",
    "# sequential = nn.Sequential(\n",
    "#     nn.Linear(in_features=input_tensor.shape[1], out_features=5),\n",
    "#     nn.Linear(in_features=5, out_features=3),\n",
    "#     nn.Linear(in_features=3, out_features=num_out),\n",
    "#     nn.ELU()\n",
    "# )\n",
    "# sequential(input_tensor)\n",
    "\n",
    "# (or)\n",
    "# sequential = nn.Sequential(\n",
    "#     nn.Linear(in_features=input_tensor.shape[1], out_features=5),\n",
    "#     nn.Linear(in_features=5, out_features=3),\n",
    "#     nn.Linear(in_features=3, out_features=num_out),\n",
    "# )\n",
    "# output = sequential(input_tensor)\n",
    "# yhat = nn.functional.elu(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization\n",
    "- Useful to handle vanishing and exploding gradient.\n",
    "- Good Initialization ensure\n",
    "    - Variance of layer inputs = variance of layer outputs\n",
    "    - Variance of gradients the same before and after a layer.\n",
    "- Methods are different for each activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2751,  0.5342, -0.5666],\n",
       "        [-0.0347, -0.4694, -0.2718],\n",
       "        [ 0.3737, -0.0626, -0.1293],\n",
       "        [ 0.3223, -0.0420, -0.2485],\n",
       "        [-0.1106,  0.4756, -0.0402]], requires_grad=True)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequential[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaiming Init:  Parameter containing:\n",
      "tensor([[-1.1691,  0.7021, -0.3884],\n",
      "        [-0.4028,  0.8705, -0.3006],\n",
      "        [-1.0375,  0.8404, -1.4055],\n",
      "        [ 1.3897, -0.0479,  0.2500],\n",
      "        [-0.1482,  0.6126, -0.1561]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# For ReLU and Semilar Activation Functions.\n",
    "# He/Kaiming Initialization\n",
    "\n",
    "init.kaiming_uniform_(sequential[0].weight)\n",
    "print(\"Kaiming Init: \", sequential[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classifictaion\n",
    "- Require one hot encoding to the truth label y.\n",
    "- use CrossEntropyLoss (torch.nn.CrossEntropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Neural network to get y_hats\n",
    "\n",
    "num_out = 3\n",
    "\n",
    "input_tensor = torch.tensor([[2.0, 9.9, -1], [2.5, 9.5, -1.5], [1.5, 8, -4]])\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=input_tensor.shape[1], out_features=5),\n",
    "    nn.Linear(in_features=5, out_features=3),\n",
    "    nn.Linear(in_features=3, out_features=num_out),\n",
    "    nn.Softmax(dim=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encoding\n",
    "num_out = 3\n",
    "y_true = torch.tensor([0, 1, 2])\n",
    "F.one_hot(y_true, num_classes=num_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model[0].weight.grad)\n",
    "print(model[1].weight.grad)\n",
    "print(model[2].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0879, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the loss\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "loss = criterion(y_hat, y_true)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gradient.\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0217,  0.0638,  0.0208],\n",
      "        [-0.0290, -0.0938, -0.0437],\n",
      "        [-0.0132, -0.0491, -0.0318],\n",
      "        [ 0.0333,  0.0890,  0.0151],\n",
      "        [-0.0512, -0.1586, -0.0637]])\n",
      "tensor([[-0.0592, -0.1516, -0.0087, -0.1133,  0.0446],\n",
      "        [ 0.0696,  0.1856,  0.0273,  0.1347, -0.0510],\n",
      "        [ 0.0007, -0.0163, -0.0414, -0.0024, -0.0039]])\n",
      "tensor([[-0.0013, -0.0278,  0.0364],\n",
      "        [-0.0488, -0.0190,  0.0527],\n",
      "        [ 0.0501,  0.0468, -0.0891]])\n"
     ]
    }
   ],
   "source": [
    "print(model[0].weight.grad)\n",
    "print(model[1].weight.grad)\n",
    "print(model[2].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating Model Parameters (Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "- loss.\n",
    "- optimizer will update the model parameters (weights) automatically. `optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f9630a3b270>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer lists\n",
    "lr = 0.01 # Learning rate.\n",
    "momentum = 0.09 # Mementum to reach the global momentum.\n",
    "\n",
    "# Stochastic Gradient Descend, simple, rarely used in practice.\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "# Adaptive Gradient\n",
    "# Good for sparse data where some features are not often observed.\n",
    "# But decreased learning too fast.\n",
    "# Cannot use momentum.\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=lr)\n",
    "\n",
    "# RMSprop - Root Mean Square Propagation.\n",
    "# Adapt the learning based on the previous gradients.\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "##################################################\n",
    "# The MOST POPULAR Method. (DEFAULT Method)\n",
    "##################################################\n",
    "\n",
    "# Adam - Adaptive moment estimation.\n",
    "# RMSProp + Momentum - most recent gradients have more weights.\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(input_tensor)\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "loss = criterion(y_hat, y_true)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Sequential(\n",
       "  (0): Linear(in_features=3, out_features=5, bias=True)\n",
       "  (1): Linear(in_features=5, out_features=3, bias=True)\n",
       "  (2): Linear(in_features=3, out_features=3, bias=True)\n",
       "  (3): Softmax(dim=-1)\n",
       ")>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3764,  0.2672,  0.4446],\n",
      "        [-0.0268,  0.1104,  0.2934],\n",
      "        [-0.5184,  0.0213,  0.4332],\n",
      "        [-0.3090,  0.5258, -0.1544],\n",
      "        [ 0.5495, -0.4032, -0.0214]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0958, -0.1698, -0.3226,  0.1997, -0.3070],\n",
      "        [ 0.3433, -0.0698, -0.2307,  0.0943, -0.3171],\n",
      "        [ 0.3749,  0.3677,  0.2880, -0.2014, -0.0077]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.5041,  0.4766, -0.4686],\n",
      "        [-0.5452, -0.5262,  0.1746],\n",
      "        [-0.1246, -0.5688, -0.2138]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model[0].weight)\n",
    "print(model[1].weight)\n",
    "print(model[2].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3764,  0.2671,  0.4447],\n",
      "        [-0.0268,  0.1105,  0.2934],\n",
      "        [-0.5183,  0.0214,  0.4331],\n",
      "        [-0.3090,  0.5257, -0.1544],\n",
      "        [ 0.5496, -0.4031, -0.0215]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0958, -0.1698, -0.3227,  0.1998, -0.3071],\n",
      "        [ 0.3434, -0.0697, -0.2304,  0.0939, -0.3170],\n",
      "        [ 0.3749,  0.3677,  0.2879, -0.2012, -0.0077]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.5043,  0.4765, -0.4684],\n",
      "        [-0.5452, -0.5262,  0.1746],\n",
      "        [-0.1245, -0.5687, -0.2139]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model[0].weight)\n",
    "print(model[1].weight)\n",
    "print(model[2].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the Number of Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 50\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "\n",
    "for parameters in model.parameters():\n",
    "    total += parameters.numel()\n",
    "\n",
    "print(f\"Total params: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "acc = Accuracy(task='binary')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_Datacamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
